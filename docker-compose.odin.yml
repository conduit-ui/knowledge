services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: knowledge-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API (optional)
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    container_name: knowledge-redis
    restart: unless-stopped
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  embeddings:
    build:
      context: ./docker/embedding-server
      dockerfile: Dockerfile
    container_name: knowledge-embeddings
    restart: unless-stopped
    ports:
      - "8001:8001"
    volumes:
      - embedding_cache:/root/.cache
    environment:
      - EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
      - DEVICE=cpu
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama runs natively on Mac with Metal GPU - no container needed
  # Access at localhost:11434

volumes:
  qdrant_storage:
    driver: local
  redis_data:
    driver: local
  embedding_cache:
    driver: local

networks:
  default:
    name: knowledge-network
